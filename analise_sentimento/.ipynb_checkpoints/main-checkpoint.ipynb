{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicti = 'arquivo/'\n",
    "f = open(dicti+'neg_tweets.txt', 'r')\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "\n",
    "tokens = tknzr.tokenize(text)\n",
    "# print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (2, 172)\t1\n",
      "  (4, 168)\t1\n",
      "  (5, 144)\t1\n",
      "  (6, 116)\t1\n",
      "  (7, 97)\t1\n",
      "  (8, 9)\t1\n",
      "  (9, 164)\t1\n",
      "  (13, 100)\t1\n",
      "  (16, 168)\t1\n",
      "  (17, 57)\t1\n",
      "  (21, 48)\t1\n",
      "  (22, 113)\t1\n",
      "  (24, 30)\t1\n",
      "  (26, 75)\t1\n",
      "  (27, 164)\t1\n",
      "  (29, 64)\t1\n",
      "  (30, 182)\t1\n",
      "  (31, 31)\t1\n",
      "  (32, 72)\t1\n",
      "  (33, 168)\t1\n",
      "  (35, 7)\t1\n",
      "  (36, 97)\t1\n",
      "  (40, 188)\t1\n",
      "  (42, 121)\t1\n",
      "  (43, 89)\t1\n",
      "  :\t:\n",
      "  (24908, 190)\t1\n",
      "  (24910, 51)\t1\n",
      "  (24911, 71)\t1\n",
      "  (24913, 42)\t1\n",
      "  (24914, 102)\t1\n",
      "  (24915, 97)\t1\n",
      "  (24920, 97)\t1\n",
      "  (24921, 39)\t1\n",
      "  (24922, 191)\t1\n",
      "  (24923, 126)\t1\n",
      "  (24932, 156)\t1\n",
      "  (24934, 116)\t1\n",
      "  (24935, 129)\t1\n",
      "  (24940, 55)\t1\n",
      "  (24941, 35)\t1\n",
      "  (24942, 97)\t1\n",
      "  (24943, 157)\t1\n",
      "  (24944, 53)\t1\n",
      "  (24945, 23)\t1\n",
      "  (24952, 190)\t1\n",
      "  (24954, 51)\t1\n",
      "  (24955, 66)\t1\n",
      "  (24956, 101)\t1\n",
      "  (24957, 133)\t1\n",
      "  (24958, 157)\t1\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    max_features = 200\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(tokens)\n",
    "\n",
    "features_nd = features.toarray()\n",
    "\n",
    "print (features)\n",
    "print (features_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
